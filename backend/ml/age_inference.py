# ml/age_inference.py
# -*- coding: utf-8 -*-
"""
–õ–æ–≥–∏–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–æ–∑—Ä–∞—Å—Ç–Ω–æ–≥–æ —Ä–µ–π—Ç–∏–Ω–≥–∞ —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤.

- –ë–µ—Ä—ë–º —Å–ø–∏—Å–æ–∫ —Å—Ü–µ–Ω (—Å—Ç—Ä–æ–∫–∏ —Ç–µ–∫—Å—Ç–∞ –∏–ª–∏ dict —Å –ø–æ–ª–µ–º 'text').
- –î–ª—è –∫–∞–∂–¥–æ–π —Å—Ü–µ–Ω—ã:
    - –õ–µ–∫—Å–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ (–ø–æ —Ä–µ–≥—É–ª—è—Ä–∫–∞–º –∏–∑ lexicons.py):
        violence / erotica / profanity / substances / scary
    - –ò–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Å—á–∏—Ç–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω–æ –¥–æ–ø—É—Å—Ç–∏–º—ã–π –≤–æ–∑—Ä–∞—Å—Ç –¥–ª—è —Å—Ü–µ–Ω—ã.
    - –ï—Å–ª–∏ –µ—Å—Ç—å –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∫–∞:
        - –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –≤–æ–∑—Ä–∞—Å—Ç –¥–ª—è —Å—Ü–µ–Ω—ã (6 / 12 / 16 / 18)
        - –ê–∫–∫—É—Ä–∞—Ç–Ω–æ –∫–æ–º–±–∏–Ω–∏—Ä—É–µ–º —Å –ª–µ–∫—Å–∏—á–µ—Å–∫–∏–º –≤–æ–∑—Ä–∞—Å—Ç–æ–º (–Ω–µ –¥–∞—ë–º NN
          –ø–æ–¥–Ω–∏–º–∞—Ç—å —á–∏—Å—Ç–æ –¥–µ—Ç—Å–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ä–∞–∑—É –¥–æ 16‚Äì18).

- –ù–∞ –≤—Å—ë –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ:
    - Aggregation –ø–æ —Å—Ü–µ–Ω–∞–º ‚Üí script_age (–ª–µ–∫—Å–∏–∫–∞), nn_script_age (NN),
      –∏—Ç–æ–≥–æ–≤—ã–π —Ä–µ–π—Ç–∏–Ω–≥ rating_int –∏ rating.
"""

from __future__ import annotations

import os
import re
from typing import Any, Dict, List, Optional, Tuple

import torch
from transformers import AutoTokenizer

from .age_model import AgeClassifier
from .lexicons import (
    VIOLENCE_PATTERNS,
    EROTICA_MILD_PATTERNS,
    EROTICA_HARD_PATTERNS,
    PROFANITY_PATTERNS,
    SUBSTANCES_MILD_PATTERNS,
    SUBSTANCES_HARD_PATTERNS,
    SCARY_PATTERNS,
)

# --------------------------- –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã ---------------------------

# –ö–∞—Ç–µ–≥–æ—Ä–∏–∏, –∫–æ—Ç–æ—Ä—ã–º–∏ –æ–ø–µ—Ä–∏—Ä—É–µ—Ç —Å–µ—Ä–≤–∏—Å –∏ —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥
CATEGORIES: List[str] = ["violence", "erotica", "profanity", "substances", "scary"]

# –£—Ä–æ–≤–Ω–∏ —Ç—è–∂–µ—Å—Ç–∏
SEVERITY_LABELS = ["none", "mild", "moderate", "severe"]

# –ú–µ—Ç–∫–∏ –≤–æ–∑—Ä–∞—Å—Ç–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ (–∏–Ω–¥–µ–∫—Å -> –≤–æ–∑—Ä–∞—Å—Ç)
# idx: 0 -> 6+, 1 -> 12+, 2 -> 16+, 3 -> 18+
AGE_LABELS: List[int] = [6, 12, 16, 18]


def severity_label(idx: int) -> str:
    if 0 <= idx < len(SEVERITY_LABELS):
        return SEVERITY_LABELS[idx]
    return "none"


# --------------------------- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ---------------------------


def _any_match(patterns: List[str], text: str) -> bool:
    """–ï—Å—Ç—å –ª–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ö–æ—Ç—è –±—ã –ø–æ –æ–¥–Ω–æ–º—É –ø–∞—Ç—Ç–µ—Ä–Ω—É."""
    for p in patterns:
        if re.search(p, text, flags=re.IGNORECASE | re.MULTILINE):
            return True
    return False


def _scene_to_text(scene: Any) -> str:
    """
    –£–Ω–∏—Ñ–∏–∫–∞—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∞ —Å—Ü–µ–Ω—ã:
    - —Å—Ç—Ä–æ–∫–∞ -> –∫–∞–∫ –µ—Å—Ç—å
    - dict -> –ø—Ä–æ–±—É–µ–º 'text' / 'scene_text'
    - –ø—Ä–æ—á–µ–µ -> str(scene)
    """
    if isinstance(scene, str):
        return scene
    if isinstance(scene, dict):
        return (
            scene.get("text")
            or scene.get("scene_text")
            or scene.get("content")
            or ""
        )
    return str(scene)


# --------------------------- –õ–µ–∫—Å–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ ---------------------------


class LexicalAnalyzer:
    """
    –û—Ç–≤–µ—á–∞–µ—Ç —Ç–æ–ª—å–∫–æ –∑–∞ —Ç–æ, —á—Ç–æ–±—ã –ø–æ —Ç–µ–∫—Å—Ç—É —Å—Ü–µ–Ω—ã:
    - –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (violence/erotica/‚Ä¶ + severity_index)
    - –≤—ã–¥–∞—Ç—å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –≤–æ–∑—Ä–∞—Å—Ç –¥–ª—è —Å—Ü–µ–Ω—ã –ø–æ –ø—Ä–∞–≤–∏–ª–∞–º
    """

    def __init__(self) -> None:
        pass

    def detect_categories(self, text: str) -> Dict[str, Dict[str, Any]]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:

        {
          "violence": {"severity_index": 2, "severity": "moderate", "confidence": 1.0},
          ...
        }
        """
        res: Dict[str, Dict[str, Any]] = {}

        # violence ‚Äî –≤—Å—ë, —á—Ç–æ –≤ VIOLENCE_PATTERNS —Å—á–∏—Ç–∞–µ–º —Ö–æ—Ç—è –±—ã "moderate"
        vio = 2 if _any_match(VIOLENCE_PATTERNS, text) else 0

        # erotica: mild / hard
        ero_mild = _any_match(EROTICA_MILD_PATTERNS, text)
        ero_hard = _any_match(EROTICA_HARD_PATTERNS, text)
        if ero_hard:
            ero = 3
        elif ero_mild:
            ero = 1
        else:
            ero = 0

        # profanity ‚Äî –≤—Å–µ —Ç–≤–æ–∏ –º–∞—Ç–Ω—ã–µ —Å–ª–æ–≤–∞/–æ—Å–∫–æ—Ä–±–ª–µ–Ω–∏—è —Å—á–∏—Ç–∞–µ–º "severe"
        prof = 3 if _any_match(PROFANITY_PATTERNS, text) else 0

        # substances: mild / hard
        sub_mild = _any_match(SUBSTANCES_MILD_PATTERNS, text)
        sub_hard = _any_match(SUBSTANCES_HARD_PATTERNS, text)
        if sub_hard:
            sub = 3
        elif sub_mild:
            sub = 1
        else:
            sub = 0

        # scary ‚Äî –≤—Å—ë, —á—Ç–æ –≤ SCARY_PATTERNS, —Å—á–∏—Ç–∞–µ–º "moderate"
        scary = 2 if _any_match(SCARY_PATTERNS, text) else 0

        mapping = {
            "violence": vio,
            "erotica": ero,
            "profanity": prof,
            "substances": sub,
            "scary": scary,
        }

        for cat in CATEGORIES:
            idx = mapping.get(cat, 0)
            res[cat] = {
                "severity_index": idx,
                "severity": severity_label(idx),
                "confidence": 1.0,  # –ø–æ regex —É –Ω–∞—Å –ª–∏–±–æ 0, –ª–∏–±–æ 1
            }

        return res

    def scene_min_age(self, categories: Dict[str, Dict[str, Any]]) -> int:
        """
        –ñ—ë—Å—Ç–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –≤ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –≤–æ–∑—Ä–∞—Å—Ç —Å—Ü–µ–Ω—ã.
        –¢—É—Ç –∫–∞–∫ —Ä–∞–∑ –º–æ–∂–Ω–æ –ø–æ–¥–∫—Ä—É—á–∏–≤–∞—Ç—å, –µ—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –∑–∞–≤—ã—à–∞–µ—Ç.
        """
        age = 6

        vio = categories["violence"]["severity_index"]
        ero = categories["erotica"]["severity_index"]
        prof = categories["profanity"]["severity_index"]
        sub = categories["substances"]["severity_index"]
        scary = categories["scary"]["severity_index"]

        # –ù–∞—Å–∏–ª–∏–µ
        if vio >= 2:
            age = max(age, 16)

        # –≠—Ä–æ—Ç–∏–∫–∞
        if ero == 1:  # –º—è–≥–∫–∞—è
            age = max(age, 12)
        elif ero >= 3:  # –∂—ë—Å—Ç–∫–∞—è
            age = max(age, 18)

        # –ú–∞—Ç / –æ—Å–∫–æ—Ä–±–ª–µ–Ω–∏—è (—É —Ç–µ–±—è —Ç–∞–º –∂—ë—Å—Ç–∫–∏–µ —Å–ª–æ–≤–∞)
        if prof >= 1:
            age = max(age, 16)

        # –í–µ—â–µ—Å—Ç–≤–∞
        if sub == 1:  # –ª—ë–≥–∫–∏–π –∞–ª–∫–æ–≥–æ–ª—å
            age = max(age, 12)
        elif sub >= 3:  # –Ω–∞—Ä–∫–æ—Ç–∏–∫–∏ / —Ç—è–∂—ë–ª—ã–µ
            age = max(age, 18)

        # –°—Ç—Ä–∞—à–∏–ª–∫–∏
        if scary >= 2:
            age = max(age, 12)

        return age


# --------------------------- –°–µ—Ä–≤–∏—Å —Å –Ω–µ–π—Ä–æ—Å–µ—Ç–∫–æ–π ---------------------------


class AgeRatingService:
    """
    –ì–ª–∞–≤–Ω—ã–π —Å–µ—Ä–≤–∏—Å: –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –ª–µ–∫—Å–∏–∫—É + –Ω–µ–π—Ä–æ—Å–µ—Ç—å –∏ —Å—á–∏—Ç–∞–µ—Ç –æ–±—â–∏–π —Ä–µ–π—Ç–∏–Ω–≥.
    """

    def __init__(self, model_dir: Optional[str] = None, use_nn: bool = True) -> None:
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"–ò—Å–ø–æ–ª—å–∑—É—é —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")

        self.lex = LexicalAnalyzer()

        # —Ñ–ª–∞–≥–∏ –∏ –æ–±—ä–µ–∫—Ç—ã NN
        self.has_age_nn: bool = False
        self.use_nn: bool = use_nn
        self.age_model: Optional[AgeClassifier] = None
        self.age_tokenizer: Optional[AutoTokenizer] = None

        if model_dir is not None and use_nn:
            self._load_age_model(model_dir)
        else:
            print("[INFO] model_dir –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω –∏–ª–∏ use_nn=False, –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–∞—è –º–æ–¥–µ–ª—å –≤–æ–∑—Ä–∞—Å—Ç–∞ –æ—Ç–∫–ª—é—á–µ–Ω–∞.")

    # --------- –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ ---------

    def _load_age_model(self, model_dir: str) -> None:
        """
        –ü—ã—Ç–∞–µ—Ç—Å—è –Ω–∞–π—Ç–∏ —á–µ–∫–ø–æ–∏–Ω—Ç –≤ model_dir:
        - —Å–Ω–∞—á–∞–ª–∞: age_checkpoint.pt (–Ω–∞—à 4-–∫–ª–∞—Å—Å–æ–≤—ã–π)
        - –¥–∞–ª–µ–µ: best_checkpoint.pt / checkpoint.pt / model.pt
        - –µ—Å–ª–∏ –Ω–µ—Ç ‚Äî –±–µ—Ä—ë–º –ø–µ—Ä–≤—ã–π *.pt
        - –µ—Å–ª–∏ –≤–æ–æ–±—â–µ –Ω–∏—á–µ–≥–æ –Ω–µ—Ç ‚Äî –ù–ï –ø–∞–¥–∞–µ–º, –ø—Ä–æ—Å—Ç–æ –æ—Ç–∫–ª—é—á–∞–µ–º NN
        """
        print(f"[AGE_NN] –ò—â—É —á–µ–∫–ø–æ–∏–Ω—Ç –º–æ–¥–µ–ª–∏ –≤–æ–∑—Ä–∞—Å—Ç–∞ –≤ –∫–∞—Ç–∞–ª–æ–≥–µ: {model_dir}")
        ckpt_candidates = [
            os.path.join(model_dir, "age_checkpoint.pt"),   # –Ω–∞—à –æ—Å–Ω–æ–≤–Ω–æ–π —á–µ–∫–ø–æ–∏–Ω—Ç
            os.path.join(model_dir, "best_checkpoint.pt"),
            os.path.join(model_dir, "checkpoint.pt"),
            os.path.join(model_dir, "model.pt"),
        ]

        ckpt_path: Optional[str] = None
        for p in ckpt_candidates:
            if os.path.isfile(p):
                ckpt_path = p
                print(f"[AGE_NN] –ù–∞–π–¥–µ–Ω —á–µ–∫–ø–æ–∏–Ω—Ç: {ckpt_path}")
                break

        # –µ—Å–ª–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –∏–º—ë–Ω –Ω–µ—Ç ‚Äî –∏—â–µ–º –ª—é–±–æ–π *.pt
        if ckpt_path is None and os.path.isdir(model_dir):
            for name in os.listdir(model_dir):
                if name.lower().endswith(".pt"):
                    ckpt_path = os.path.join(model_dir, name)
                    print(f"[AGE_NN] –ù–∞–π–¥–µ–Ω –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç: {ckpt_path}")
                    break

        if ckpt_path is None:
            print(
                f"[WARN] –í –∫–∞—Ç–∞–ª–æ–≥–µ {model_dir!r} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ *.pt. "
                f"–ù–µ–π—Ä–æ—Å–µ—Ç–µ–≤–∞—è –º–æ–¥–µ–ª—å –≤–æ–∑—Ä–∞—Å—Ç–∞ –±—É–¥–µ—Ç –æ—Ç–∫–ª—é—á–µ–Ω–∞.\n"
                f"–ß—Ç–æ–±—ã –≤–∫–ª—é—á–∏—Ç—å –µ—ë —Å–Ω–æ–≤–∞, –∑–∞–ø—É—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏–µ, –Ω–∞–ø—Ä–∏–º–µ—Ä:\n"
                f"  python -m ml.train_age --csv data/age_scenes.csv "
                f"--epochs 3 --output_dir {model_dir}"
            )
            self.age_model = None
            self.age_tokenizer = None
            self.has_age_nn = False
            return

        print(f"[AGE_NN] –ó–∞–≥—Ä—É–∂–∞—é —Å–æ—Å—Ç–æ—è–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏–∑ {ckpt_path}")
        checkpoint = torch.load(ckpt_path, map_location=self.device)

        # –ø–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        state_dict = checkpoint
        if isinstance(checkpoint, dict):
            if "model_state_dict" in checkpoint:
                state_dict = checkpoint["model_state_dict"]
            elif "state_dict" in checkpoint:
                state_dict = checkpoint["state_dict"]

        model = AgeClassifier(num_labels=len(AGE_LABELS))
        model.load_state_dict(state_dict)
        model.to(self.device)
        model.eval()

        tokenizer = AutoTokenizer.from_pretrained("DeepPavlov/rubert-base-cased")

        self.age_model = model
        self.age_tokenizer = tokenizer
        self.has_age_nn = True

        print("[AGE_NN] –ù–µ–π—Ä–æ—Å–µ—Ç–µ–≤–∞—è –º–æ–¥–µ–ª—å –≤–æ–∑—Ä–∞—Å—Ç–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")

    # --------- –ë–∞—Ç—á–µ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ NN ---------

    def _predict_batch_ages_nn(
        self, texts: List[str], batch_size: int = 8
    ) -> List[Tuple[Optional[int], Optional[float], Optional[int]]]:
        """
        –ë–∞—Ç—á–µ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Å—Ü–µ–Ω.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (age_int, confidence, age_label_idx)
        —Ç–∞–∫–æ–π –∂–µ –¥–ª–∏–Ω—ã, –∫–∞–∫ texts.
        """
        if not self.use_nn or not self.has_age_nn or self.age_model is None or self.age_tokenizer is None:
            return [(None, None, None)] * len(texts)

        results: List[Tuple[Optional[int], Optional[float], Optional[int]]] = []
        total = len(texts)
        idx = 0

        while idx < total:
            batch_texts = [t for t in texts[idx: idx + batch_size]]

            # –µ—Å–ª–∏ –≤ –±–∞—Ç—á–µ —Ç–æ–ª—å–∫–æ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ ‚Äî —Å—Ä–∞–∑—É None
            if all(not t.strip() for t in batch_texts):
                results.extend([(None, None, None)] * len(batch_texts))
                idx += batch_size
                continue

            enc = self.age_tokenizer(
                batch_texts,
                padding=True,
                truncation=True,
                max_length=256,
                return_tensors="pt",
            )

            inputs = {
                "input_ids": enc["input_ids"].to(self.device),
                "attention_mask": enc["attention_mask"].to(self.device),
            }

            with torch.no_grad():
                logits = self.age_model(**inputs)  # (batch, num_labels)
                probs = torch.softmax(logits, dim=-1)
                conf_tensors, idx_tensors = torch.max(probs, dim=-1)

            for conf_tensor, idx_tensor, text in zip(conf_tensors, idx_tensors, batch_texts):
                if not text.strip():
                    results.append((None, None, None))
                    continue

                age_label_idx = int(idx_tensor.item())
                confidence = float(conf_tensor.item())
                age_int = AGE_LABELS[age_label_idx]
                results.append((age_int, confidence, age_label_idx))

            print(f"[AGE_NN] –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ü–µ–Ω: {min(idx + batch_size, total)} / {total}")
            idx += batch_size

        return results

    # --------- –û–¥–∏–Ω–æ—á–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ (–µ—Å–ª–∏ –≤–¥—Ä—É–≥ –≥–¥–µ-—Ç–æ –Ω—É–∂–Ω–æ) ---------

    def _predict_scene_age_nn(
        self, text: str
    ) -> Tuple[Optional[int], Optional[float], Optional[int]]:
        """
        –û—Å—Ç–∞–≤–∏–ª –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π, –Ω–æ –≤ analyze_script –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞—Ç—á–µ–≤—ã–π –≤–∞—Ä–∏–∞–Ω—Ç.
        """
        if not self.use_nn or not self.has_age_nn or self.age_model is None or self.age_tokenizer is None:
            return None, None, None

        if not text.strip():
            return None, None, None

        enc = self.age_tokenizer(
            text,
            padding="max_length",
            truncation=True,
            max_length=256,
            return_tensors="pt",
        )

        inputs = {
            "input_ids": enc["input_ids"].to(self.device),
            "attention_mask": enc["attention_mask"].to(self.device),
        }

        with torch.no_grad():
            logits = self.age_model(**inputs)
            probs = torch.softmax(logits, dim=-1)[0]
            conf_tensor, idx_tensor = torch.max(probs, dim=-1)

        age_label_idx = int(idx_tensor.item())
        confidence = float(conf_tensor.item())
        age_int = AGE_LABELS[age_label_idx]

        return age_int, confidence, age_label_idx

    # --------- –ö–æ–º–±–∏–Ω–∞—Ü–∏—è –ª–µ–∫—Å–∏—á–µ—Å–∫–æ–≥–æ –≤–æ–∑—Ä–∞—Å—Ç–∞ –∏ NN ---------

    @staticmethod
    def _combine_ages(lex_age: int, nn_age: Optional[int]) -> int:
        """
        –ö–∞–∫ –∫–æ–º–±–∏–Ω–∏—Ä—É–µ–º:

        - –ï—Å–ª–∏ NN –Ω–µ—Ç ‚Üí –≤–æ–∑–≤—Ä–∞—â–∞–µ–º lex_age.
        - –ï—Å–ª–∏ NN <= lex_age ‚Üí –¥–æ–≤–µ—Ä—è–µ–º –ª–µ–∫—Å–∏–∫–µ (lex_age).
        - –ï—Å–ª–∏ NN > lex_age:
            * –µ—Å–ª–∏ lex_age >= 16 ‚Üí –¥–æ–≤–µ—Ä—è–µ–º NN –ø–æ–ª–Ω–æ—Å—Ç—å—é;
            * –µ—Å–ª–∏ lex_age == 12 ‚Üí –ø–æ–∑–≤–æ–ª—è–µ–º –ø–æ–¥–Ω—è—Ç—å –º–∞–∫—Å–∏–º—É–º –¥–æ 16;
            * –µ—Å–ª–∏ lex_age == 6 ‚Üí –ø–æ–∑–≤–æ–ª—è–µ–º –ø–æ–¥–Ω—è—Ç—å –º–∞–∫—Å–∏–º—É–º –¥–æ 12
              (–Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –¥–µ–ª–∞–µ–º 16/18 —Ç–æ–ª—å–∫–æ –∏–∑-–∑–∞ NN –Ω–∞ –¥–µ—Ç—Å–∫–æ–º —Ç–µ–∫—Å—Ç–µ).
        """
        if nn_age is None:
            return lex_age

        if nn_age <= lex_age:
            return lex_age

        if lex_age >= 16:
            return nn_age

        if lex_age == 12:
            return min(nn_age, 16)

        if lex_age == 6:
            return min(nn_age, 12)

        return max(lex_age, nn_age)

    # --------- –ü—É–±–ª–∏—á–Ω—ã–π –º–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏–∑–∞ ---------

        # --------- –ü—É–±–ª–∏—á–Ω—ã–π –º–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏–∑–∞ ---------

    def analyze_script(
        self, scenes: List[Any], filename: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        –ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –∑–æ–≤—ë—Ç —Å–µ—Ä–≤–∏—Å:

        scenes ‚Äî —Å–ø–∏—Å–æ–∫ —Å—Ü–µ–Ω (—Å—Ç—Ä–æ–∫–∞ –∏–ª–∏ dict —Å 'text').

        –î–µ–ª–∞–µ—Ç:
        - –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é —Å–ø–∏—Å–∫–∞ —Å—Ü–µ–Ω (–æ–±—Ä–µ–∑–∞–µ–º –º—É—Å–æ—Ä, —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∫—É—Å–∫–∏)
        - –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Å–∂–∏–º–∞–µ–º —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ü–µ–Ω –≤ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –±–ª–æ–∫–∏

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict:
        {
          "rating": "12+",
          "rating_int": 12,
          "scenes_total": ...,
          "scenes_with_violations": ...,
          "per_category": {...},
          "problem_scenes": [...],
          "script_age": ...,
          "lex_age": ...,
          "nn_script_age": ...,
          "scene_results": [...]
        }
        """
        # ---------- –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø –°–¶–ï–ù ----------

        MIN_SCENE_LEN = 30       # –∫–∞–∫ –≤ build_age_dataset
        MAX_SCENES = 2000        # –∂—ë—Å—Ç–Ω—ã–π –≤–µ—Ä—Ö–Ω–∏–π –ø—Ä–µ–¥–µ–ª, —á—Ç–æ–±—ã –Ω–µ —É–±–∏—Ç—å –º–æ–¥–µ–ª—å—é –º–∞—à–∏–Ω—É

        # 1) –ø—Ä–∏–≤–æ–¥–∏–º –≤—Å—ë –∫ —Ç–µ–∫—Å—Ç—É + –≤—ã–∫–∏–¥—ã–≤–∞–µ–º –ø—É—Å—Ç–æ–µ/–æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–æ–µ
        normalized_scenes: List[str] = []
        for scene in scenes:
            text = _scene_to_text(scene).strip()
            if len(text) < MIN_SCENE_LEN:
                continue
            normalized_scenes.append(text)

        if not normalized_scenes:
            # –≤–æ–æ–±—â–µ –Ω–∏—á–µ–≥–æ –∞–¥–µ–∫–≤–∞—Ç–Ω–æ–≥–æ –Ω–µ –Ω–∞—à–ª–∏
            return {
                "rating": "6+",
                "rating_int": 6,
                "scenes_total": 0,
                "scenes_with_violations": 0,
                "per_category": {cat: {
                    "max_severity_index": 0,
                    "max_severity": severity_label(0),
                    "episodes": 0,
                    "scene_percent": 0.0,
                } for cat in CATEGORIES},
                "problem_scenes": [],
                "script_age": 6,
                "lex_age": 6,
                "nn_script_age": None,
                "scene_results": [],
                **({"filename": filename} if filename is not None else {}),
            }

        # 2) –µ—Å–ª–∏ —Å—Ü–µ–Ω –û–ß–ï–ù–¨ –º–Ω–æ–≥–æ ‚Äî —Å–∂–∏–º–∞–µ–º –≤ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –±–ª–æ–∫–∏
        if len(normalized_scenes) > MAX_SCENES:
            print(f"[AGE] –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —Å—Ü–µ–Ω ({len(normalized_scenes)}). "
                  f"–û–±—ä–µ–¥–∏–Ω—è—é –∏—Ö –ø—Ä–∏–º–µ—Ä–Ω–æ –≤ {MAX_SCENES} –±–ª–æ–∫–æ–≤.")
            chunk_size = (len(normalized_scenes) + MAX_SCENES - 1) // MAX_SCENES
            merged: List[str] = []
            buf: List[str] = []
            for i, t in enumerate(normalized_scenes, 1):
                buf.append(t)
                if i % chunk_size == 0:
                    merged.append("\n\n".join(buf))
                    buf = []
            if buf:
                merged.append("\n\n".join(buf))
            normalized_scenes = merged

        # –¢–µ–ø–µ—Ä—å scenes ‚Äî —É–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫
        scenes = normalized_scenes

        # ---------- –î–ê–õ–¨–®–ï –°–¢–ê–†–ê–Ø –õ–û–ì–ò–ö–ê, –¢–û–õ–¨–ö–û –ß–£–¢–¨ –ü–û–î–†–ï–ó–ê–ù–ù–ê–Ø –ü–û–î –ù–û–í–´–ï scenes ----------

        scene_results: List[Dict[str, Any]] = []

        lex_scene_ages: List[int] = []
        nn_scene_ages: List[int] = []

        for i, text in enumerate(scenes):
            # 1) –õ–µ–∫—Å–∏–∫–∞
            cats = self.lex.detect_categories(text)
            lex_age = self.lex.scene_min_age(cats)
            lex_scene_ages.append(lex_age)

            # 2) –ù–µ–π—Ä–æ—Å–µ—Ç—å
            nn_age_int, nn_conf, nn_idx = self._predict_scene_age_nn(text)
            if nn_age_int is not None:
                nn_scene_ages.append(nn_age_int)

            # 3) –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–æ–∑—Ä–∞—Å—Ç –¥–ª—è —Å—Ü–µ–Ω—ã
            combined_age = self._combine_ages(lex_age, nn_age_int)

            # confidence —É –≤–æ–∑—Ä–∞—Å—Ç–∞ —Å—Ü–µ–Ω—ã: –ª–∏–±–æ NN, –ª–∏–±–æ 1.0 (–µ—Å–ª–∏ NN –Ω–µ—Ç)
            age_conf = float(nn_conf) if nn_conf is not None else 1.0
            age_idx_for_scene = (
                nn_idx if nn_idx is not None else AGE_LABELS.index(combined_age)
            )

            scene_results.append(
                {
                    "categories": cats,
                    "scene_age": combined_age,
                    "age_confidence": age_conf,
                    "age_label_idx": int(age_idx_for_scene),
                    "scene_id": i,
                    "text_snippet": text[:1000],
                }
            )

        # --------- –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –≤—Å–µ–º—É —Å—Ü–µ–Ω–∞—Ä–∏—é ---------

        scenes_total = len(scene_results)
        scenes_with_violations = sum(
            1
            for s in scene_results
            if any(
                s["categories"][cat]["severity_index"] > 0 for cat in CATEGORIES
            )
        )

        # per_category
        per_category: Dict[str, Dict[str, Any]] = {}
        for cat in CATEGORIES:
            severities = [s["categories"][cat]["severity_index"] for s in scene_results]
            max_sev = max(severities) if severities else 0
            episodes = sum(1 for v in severities if v > 0)
            scene_percent = float(episodes / scenes_total) if scenes_total > 0 else 0.0

            per_category[cat] = {
                "max_severity_index": max_sev,
                "max_severity": severity_label(max_sev),
                "episodes": episodes,
                "scene_percent": scene_percent,
            }

                # script_age –∏ lex_age ‚Äî —á–∏—Å—Ç–æ –ø–æ –ª–µ–∫—Å–∏–∫–µ (–º–∞–∫—Å–∏–º—É–º –ø–æ —Å—Ü–µ–Ω–∞–º)
        lex_age_script = max(lex_scene_ages) if lex_scene_ages else 6
        script_age = lex_age_script

        # nn_script_age ‚Äî –º–∞–∫—Å–∏–º—É–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π NN –ø–æ —Å—Ü–µ–Ω–∞–º (–µ—Å–ª–∏ –µ—Å—Ç—å)
        nn_script_age: Optional[int]
        if nn_scene_ages:
            nn_script_age = max(nn_scene_ages)
        else:
            nn_script_age = None

        # –∏—Ç–æ–≥–æ–≤—ã–π –≤–æ–∑—Ä–∞—Å—Ç —Å—Ü–µ–Ω–∞—Ä–∏—è (combination)
        final_age_int = self._combine_ages(script_age, nn_script_age)

        # üî• –í–ê–ñ–ù–û: –µ—Å–ª–∏ –ª–µ–∫—Å–∏–∫–∞ –≥–æ–≤–æ—Ä–∏—Ç "—á–∏—Å—Ç—ã–π 6+" –∏ –ù–ï–¢ –Ω–∞—Ä—É—à–µ–Ω–∏–π,
        # –Ω–µ –¥–∞—ë–º –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –∑–∞–¥–∏—Ä–∞—Ç—å —Ä–µ–π—Ç–∏–Ω–≥
        if script_age == 6 and scenes_with_violations == 0:
            final_age_int = 6

        rating_str = f"{final_age_int}+"

        # problem_scenes ‚Äî –≤—Å–µ —Å—Ü–µ–Ω—ã, –≥–¥–µ –µ—Å—Ç—å –Ω–∞—Ä—É—à–µ–Ω–∏—è (severity > 0),
        # –ø–ª—é—Å —Å—Ü–µ–Ω—ã —Å –≤–æ–∑—Ä–∞—Å—Ç–æ–º > 6
        problem_scenes: List[Dict[str, Any]] = []
        for s in scene_results:
            has_violation = any(
                s["categories"][cat]["severity_index"] > 0 for cat in CATEGORIES
            )
            if has_violation or s["scene_age"] > 6:
                issues = []
                for cat in CATEGORIES:
                    sev_idx = s["categories"][cat]["severity_index"]
                    if sev_idx > 0:
                        issues.append(
                            {
                                "category": cat,
                                "severity": s["categories"][cat]["severity"],
                                "severity_index": sev_idx,
                                "confidence": s["categories"][cat]["confidence"],
                            }
                        )
                problem_scenes.append(
                    {
                        "scene_id": s["scene_id"],
                        "scene_age": s["scene_age"],
                        "issues": issues,
                        "text_snippet": s["text_snippet"],
                    }
                )

        result: Dict[str, Any] = {
            "rating": rating_str,
            "rating_int": final_age_int,
            "scenes_total": scenes_total,
            "scenes_with_violations": scenes_with_violations,
            "per_category": per_category,
            "problem_scenes": problem_scenes,
            "script_age": script_age,
            "lex_age": lex_age_script,
            "nn_script_age": nn_script_age,
            "scene_results": scene_results,
        }

        if filename is not None:
            result["filename"] = filename

        return result

